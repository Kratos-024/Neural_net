{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e61fed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18f23594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02652128, -0.0298474 , -0.00894195]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([1,2,3])\n",
    "weights = np.random.randn(3, 3) * 0.01\n",
    "bias = np.zeros((1,3))\n",
    "\n",
    "output=np.dot(input,weights)+bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f58487",
   "metadata": {},
   "source": [
    "Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c51f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluAct:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,inputs):\n",
    "         self.output=np.maximum(0,inputs)\n",
    "         return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c2ddb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([1,2,3])\n",
    "# weights = np.random.randn(3, 3) * 0.01\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "366010d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# weights = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ae984",
   "metadata": {},
   "source": [
    "# Batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00df55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# weights = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276db529",
   "metadata": {},
   "source": [
    "# Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e095d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# # Layer 1\n",
    "# weights1 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias1 = np.zeros((1,3))\n",
    "\n",
    "# # Layer 2\n",
    "# weights2 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias2 = np.zeros((1,3))\n",
    "\n",
    "# layer1=np.dot(input,weights1)+bias1\n",
    "# layer1 = ReLu(layer1)\n",
    "# print(layer1)\n",
    "\n",
    "# layer2=np.dot(layer1,weights2)+bias2\n",
    "# output = ReLu(layer2)\n",
    "\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998c434",
   "metadata": {},
   "source": [
    "# Class of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a98afd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights=0.01*np.random.randn(inputs,neurons)\n",
    "        self.bias=np.zeros((1,neurons))\n",
    "    def forward(self,input):\n",
    "        self.output = np.dot(input,self.weights)+self.bias\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551aa9",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9e09268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        max_row = np.max(input,axis=1,keepdims=True)\n",
    "        normalize_row = input-max_row\n",
    "        expo_max = np.sum(np.exp(normalize_row),axis=1,keepdims=True)\n",
    "        expo_values = np.exp(normalize_row)/expo_max\n",
    "        self.output = expo_values\n",
    "        return self.output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f6c47",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7cf746ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_entropyLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1.0)\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "       \n",
    "        self.output = np.mean(sample_losses)\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c67c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([\n",
    "    [1, 0, 0],  # sample 1 → class 0\n",
    "    [0, 1, 0],  # sample 2 → class 1\n",
    "    [0, 0, 1]   # sample 3 → class 2\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87508809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(19.63061671445764)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = Cross_entropyLoss()\n",
    "cs.forward(output,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d6ccc",
   "metadata": {},
   "source": [
    "# Dense Layer with Activation, Softmax, Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24b5747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0981175686269748\n"
     ]
    }
   ],
   "source": [
    "input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# Layer 1\n",
    "weights1 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "bias1 = np.zeros((1,3))\n",
    "\n",
    "# Layer 2\n",
    "weights2 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "bias2 = np.zeros((1,3))\n",
    "\n",
    "layer1=np.dot(input,weights1)+bias1\n",
    "# layer1 = ReLu(layer1)\n",
    "\n",
    "layer2=np.dot(layer1,weights2)+bias2\n",
    "\n",
    "actSoft = Softmax()\n",
    "actSoft.forward(layer2)\n",
    "\n",
    "lossFun = Cross_entropyLoss()\n",
    "lossFun.forward(actSoft.output,[[1,0,0],[0,1,0],[1,0,0]])\n",
    "\n",
    "\n",
    "print(lossFun.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Less loss has been found at:  0  loss  1.098389928761526  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  1  loss  1.0981048242597875  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  2  loss  1.0978748896363886  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  3  loss  1.0974668922892004  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  7  loss  1.0971870979007774  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  37  loss  1.0967952808769545  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  38  loss  1.0966299453373018  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  77  loss  1.0930602296942922  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  78  loss  1.0908021935933274  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  79  loss  1.089006901236034  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  80  loss  1.0876429992075112  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  82  loss  1.0872302920657084  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  83  loss  1.087171928275835  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  84  loss  1.086278890142195  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  89  loss  1.0846542325019612  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  109  loss  1.0841353272262961  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  111  loss  1.0817014337763953  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  113  loss  1.0789075247729942  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  115  loss  1.0787889319416035  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  116  loss  1.072684314219577  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  122  loss  1.070830013326858  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  123  loss  1.0674657978496074  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  124  loss  1.0643545808073311  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  125  loss  1.061824340619727  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  127  loss  1.0595808685641264  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  751  loss  1.0590930480779286  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  771  loss  1.0585641057135737  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  778  loss  1.0568559227591245  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  780  loss  1.05243752617836  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  783  loss  1.0457488325624622  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  784  loss  1.0450784683304175  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  785  loss  1.0354053434881967  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  786  loss  1.030408123231702  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  848  loss  1.0211081482576103  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  850  loss  1.0159033387586498  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  851  loss  0.9967754748986888  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  852  loss  0.9884488896467403  Accuracy  0.3333333333333333\n",
      "New Less loss has been found at:  854  loss  0.9756374443126848  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  865  loss  0.9545783372955055  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  868  loss  0.9513227134515644  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  879  loss  0.9464050748886134  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  885  loss  0.943885977522935  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  886  loss  0.9299872337308059  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2536  loss  0.9241159265896254  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2540  loss  0.9193359890180339  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2541  loss  0.9136855667325067  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2542  loss  0.9086429638265313  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2543  loss  0.9057822147378162  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2557  loss  0.9019436391034894  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2558  loss  0.890376070189622  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2559  loss  0.8850016035773324  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2560  loss  0.8745209943901521  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2564  loss  0.8702724766451091  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2576  loss  0.8681821999168106  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2577  loss  0.8600023606139983  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2578  loss  0.8582904842815001  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2579  loss  0.8486418381390085  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2580  loss  0.843937919464138  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2581  loss  0.8438458896773081  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2584  loss  0.8368581608316995  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2585  loss  0.826959009432693  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2587  loss  0.8228096727308204  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2604  loss  0.8214382231178495  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2606  loss  0.8100519754256271  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2617  loss  0.8076674942936606  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2618  loss  0.8067568582008415  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2659  loss  0.8061694814731748  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2668  loss  0.8049311736860713  Accuracy  0.6666666666666666\n",
      "New Less loss has been found at:  2669  loss  0.8036521640018447  Accuracy  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [3, 4, 5],\n",
    "    [6, 8, 1]\n",
    "])\n",
    "\n",
    "# True labels (3 classes, one-hot)\n",
    "# Let's say sample 1 → class 0, sample 2 → class 1, sample 3 → class 2\n",
    "y = np.array([\n",
    "    [1, 0, 0],  # sample 1 → class 0\n",
    "    [0, 1, 0],  # sample 2 → class 1\n",
    "    [0, 0, 1]   # sample 3 → class 2\n",
    "])\n",
    "\n",
    "layer1 = Dense(3,3)\n",
    "layer2 = Dense(3,3)\n",
    "\n",
    "largest_loss = 99999\n",
    "best_weights1 = layer1.weights.copy()\n",
    "best_bias1 = layer1.bias.copy()\n",
    "best_weights2 = layer2.weights.copy()\n",
    "best_bias2 = layer2.bias.copy()\n",
    "\n",
    "reluAct = ReluAct()\n",
    "actSoft = Softmax()\n",
    "lossFun = Cross_entropyLoss()\n",
    "\n",
    "for i in range(1000000):\n",
    "    layer1.weights -= 0.01 * np.random.randn(3,3)\n",
    "    layer1.bias -= np.zeros((1,3))\n",
    "    layer2.weights -= 0.01 *np.random.randn(3,3)\n",
    "    layer2.bias -= np.zeros((1,3))\n",
    "\n",
    "    output_layer1 = layer1.forward(X)\n",
    "    output_relu = reluAct.forward(output_layer1)\n",
    "    output_layer2 = layer2.forward(output_relu)\n",
    "    actSoft_output = actSoft.forward(output_layer2)\n",
    "    lossFun_output = lossFun.forward(actSoft_output,y)\n",
    "    \n",
    "    # actSoft_output.output has shape (batch_size, n_classes)\n",
    "    pred_classes = np.argmax(actSoft_output, axis=1)\n",
    "    true_classes = np.argmax(y, axis=1)\n",
    "\n",
    "    acc = np.mean(pred_classes == true_classes)\n",
    "\n",
    "    \n",
    "    if lossFun_output < largest_loss:\n",
    "        print(\"New Less loss has been found at: \",i, \" loss \",lossFun_output, \" Accuracy \", acc)\n",
    "        largest_loss=lossFun_output\n",
    "        best_weights1 = layer1.weights.copy()\n",
    "        best_bias1 = layer1.bias.copy()\n",
    "        best_weights2 = layer2.weights.copy()\n",
    "        best_bias2 = layer2.bias.copy()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8296e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
