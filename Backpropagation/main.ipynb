{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f41933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data,vertical_data\n",
    "import nnfs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nnfs.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X,y= spiral_data(samples=100,classes=3)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()\n",
    "plt.scatter(X[:,0],X[:,1],c=y,cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc74f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dense:\n",
    "    def __init__(self,inputs, neurons):\n",
    "        self.weights = 0.01*np.random.randn(inputs,neurons)\n",
    "        self.bias = np.zeros((1, neurons))\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        self.outputs = np.dot(inputs,(self.weights)) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86015905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Activation_ReLu:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def forward(self,inputs):\n",
    "        self.outputs= np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2961398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer  = Dense(2,3)\n",
    "layer.forward(X)\n",
    "layer.outputs.shape\n",
    "layer.outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLu = Activation_ReLu()\n",
    "ReLu.forward(layer.outputs)\n",
    "ReLu.outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2=Dense(ReLu.outputs.shape[1],3)\n",
    "layer2.forward(ReLu.outputs)\n",
    "layer2.outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer2.outputs)\n",
    "softmax.output[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10406cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = np.array([[0.7,0.1,0.1],[0.1,.5,.4],[.02,.9,.08]])\n",
    "true_val = [0,1,1]\n",
    "print(softmax_outputs[[0,1,2], true_val])\n",
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)), true_val]))\n",
    "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), true_val])\n",
    "avg_loss = np.mean(neg_log)\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_val = [[1,0,0],[0,1,0],[0,1,0]]\n",
    "true_val = [0,1,1]\n",
    "print(np.sum(true_val*softmax_outputs,axis=1))\n",
    "neg_log = -np.log(np.sum(true_val*softmax_outputs,axis=1))\n",
    "avg_loss = np.mean(neg_log)\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a698ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Loss_CrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n",
    "        else:\n",
    "            correct_confidences = np.sum(y_true * y_pred_clipped, axis=1)\n",
    "\n",
    "        neg_log = -np.log(correct_confidences)\n",
    "        self.avg_loss = np.mean(neg_log)\n",
    "        return self.avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "softmax.output\n",
    "entropy=Loss_CrossEntropy()\n",
    "entropy.forward(softmax.output,y)\n",
    "entropy.avg_loss\n",
    "softmax_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb008e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Dense(X.shape[1], 3)\n",
    "layer1.forward(X)\n",
    "\n",
    "relu = Activation_ReLu()\n",
    "relu.forward(layer1.outputs)\n",
    "\n",
    "layer2 = Dense(layer1.outputs.shape[1], 3)\n",
    "layer2.forward(relu.outputs)\n",
    "\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer2.outputs)\n",
    "\n",
    "crossEntropy = Loss_CrossEntropy()\n",
    "loss = crossEntropy.forward(softmax.output, y)\n",
    "\n",
    "lowest_loss=999999\n",
    "best_dense1_weights=layer1.weights\n",
    "best_dense1_bias=layer1.bias\n",
    "best_dense2_weights=layer2.weights\n",
    "best_dense1_bias=layer2.bias\n",
    "for i in range(10000000):\n",
    "    layer1.weights = 0.05*np.random.randn(2,3)\n",
    "    layer1.bias = 0.05*np.random.randn(1,3)\n",
    "    layer2.weights = 0.05*np.random.randn(3,3)\n",
    "    layer2.bias = 0.05*np.random.randn(1,3)\n",
    "\n",
    "    layer1.forward(X)\n",
    "    relu.forward(layer1.outputs)\n",
    "    layer2.forward(relu.outputs)\n",
    "    softmax.forward(layer2.outputs)\n",
    "    new_loss = crossEntropy.forward(softmax.output, y)\n",
    "\n",
    "\n",
    "    y_pred = np.argmax(softmax.output, axis=1)\n",
    "    accuracy = np.mean(y == y_pred)\n",
    "    if new_loss < lowest_loss:\n",
    "        print(\"A new loss is detect with best weights and bias \",i, \" Accuracy: \",accuracy)\n",
    "        best_dense1_weights=layer1.weights.copy()\n",
    "        best_dense1_bias=layer1.bias.copy()\n",
    "        best_dense2_weights=layer2.weights.copy()\n",
    "        best_dense1_bias=layer2.bias.copy()\n",
    "        lowest_loss=new_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49500f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Dense(X.shape[1], 3)\n",
    "layer1.forward(X)\n",
    "\n",
    "relu = Activation_ReLu()\n",
    "relu.forward(layer1.outputs)\n",
    "\n",
    "layer2 = Dense(layer1.outputs.shape[1], 3)\n",
    "layer2.forward(relu.outputs)\n",
    "\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer2.outputs)\n",
    "\n",
    "crossEntropy = Loss_CrossEntropy()\n",
    "loss = crossEntropy.forward(softmax.output, y)\n",
    "\n",
    "lowest_loss = 999999\n",
    "best_dense1_weights = layer1.weights.copy()\n",
    "best_dense1_bias = layer1.bias.copy()\n",
    "best_dense2_weights = layer2.weights.copy()\n",
    "best_dense2_bias = layer2.bias.copy()\n",
    "\n",
    "for i in range(10000000):\n",
    "    layer1.weights += 0.05*np.random.randn(2,3)\n",
    "    layer1.bias += 0.05*np.random.randn(3)\n",
    "    layer2.weights += 0.05*np.random.randn(3,3)\n",
    "    layer2.bias += 0.05*np.random.randn(3)\n",
    "\n",
    "    layer1.forward(X)\n",
    "    relu.forward(layer1.outputs)\n",
    "    layer2.forward(relu.outputs)\n",
    "    softmax.forward(layer2.outputs)\n",
    "    new_loss = crossEntropy.forward(softmax.output, y)\n",
    "\n",
    "    y_pred = np.argmax(softmax.output, axis=1)\n",
    "    accuracy = np.mean(y == y_pred)\n",
    "\n",
    "    if new_loss < lowest_loss:\n",
    "        print(f\"Iteration {i}: loss {new_loss:.4f}, accuracy {accuracy:.4f}\")\n",
    "        best_dense1_weights = layer1.weights.copy()\n",
    "        best_dense1_bias = layer1.bias.copy()\n",
    "        best_dense2_weights = layer2.weights.copy()\n",
    "        best_dense2_bias = layer2.bias.copy()\n",
    "        lowest_loss = new_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,y=vertical_data(samples=100,classes=3)\n",
    "layer1 = Dense(X.shape[1], 3)\n",
    "layer1.forward(X)\n",
    "\n",
    "relu = Activation_ReLu()\n",
    "relu.forward(layer1.outputs)\n",
    "\n",
    "layer2 = Dense(layer1.outputs.shape[1], 3)\n",
    "layer2.forward(relu.outputs)\n",
    "\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer2.outputs)\n",
    "\n",
    "crossEntropy = Loss_CrossEntropy()\n",
    "loss = crossEntropy.forward(softmax.output, y)\n",
    "\n",
    "lowest_loss = 999999\n",
    "best_dense1_weights = layer1.weights.copy()\n",
    "best_dense1_bias = layer1.bias.copy()\n",
    "best_dense2_weights = layer2.weights.copy()\n",
    "best_dense2_bias = layer2.bias.copy()\n",
    "\n",
    "for i in range(10000000):\n",
    "    layer1.weights += 0.05*np.random.randn(2,3)\n",
    "    layer1.bias += 0.05*np.random.randn(3)\n",
    "    layer2.weights += 0.05*np.random.randn(3,3)\n",
    "    layer2.bias += 0.05*np.random.randn(3)\n",
    "\n",
    "    layer1.forward(X)\n",
    "    relu.forward(layer1.outputs)\n",
    "    layer2.forward(relu.outputs)\n",
    "    softmax.forward(layer2.outputs)\n",
    "    new_loss = crossEntropy.forward(softmax.output, y)\n",
    "\n",
    "    y_pred = np.argmax(softmax.output, axis=1)\n",
    "    accuracy = np.mean(y == y_pred)\n",
    "\n",
    "    if new_loss < lowest_loss:\n",
    "        print(f\"Iteration {i}: loss {new_loss:.4f}, accuracy {accuracy:.4f}\")\n",
    "        best_dense1_weights = layer1.weights.copy()\n",
    "        best_dense1_bias = layer1.bias.copy()\n",
    "        best_dense2_weights = layer2.weights.copy()\n",
    "        best_dense2_bias = layer2.bias.copy()\n",
    "        lowest_loss = new_loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
