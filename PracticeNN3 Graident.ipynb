{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e61fed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18f23594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02652128, -0.0298474 , -0.00894195]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([1,2,3])\n",
    "weights = np.random.randn(3, 3) * 0.01\n",
    "bias = np.zeros((1,3))\n",
    "\n",
    "output=np.dot(input,weights)+bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f58487",
   "metadata": {},
   "source": [
    "Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c51f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluAct:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,inputs):\n",
    "         self.output=np.maximum(0,inputs)\n",
    "         return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c2ddb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([1,2,3])\n",
    "# weights = np.random.randn(3, 3) * 0.01\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "366010d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# weights = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ae984",
   "metadata": {},
   "source": [
    "# Batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00df55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# weights = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276db529",
   "metadata": {},
   "source": [
    "# Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e095d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# # Layer 1\n",
    "# weights1 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias1 = np.zeros((1,3))\n",
    "\n",
    "# # Layer 2\n",
    "# weights2 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias2 = np.zeros((1,3))\n",
    "\n",
    "# layer1=np.dot(input,weights1)+bias1\n",
    "# layer1 = ReLu(layer1)\n",
    "# print(layer1)\n",
    "\n",
    "# layer2=np.dot(layer1,weights2)+bias2\n",
    "# output = ReLu(layer2)\n",
    "\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998c434",
   "metadata": {},
   "source": [
    "# Class of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98afd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights=0.01*np.random.randn(inputs,neurons)\n",
    "        self.bias=np.zeros((1,neurons))\n",
    "    def forward(self,input):\n",
    "        self.output = np.dot(input,self.weights)+self.bias\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551aa9",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9e09268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        max_row = np.max(input,axis=1,keepdims=True)\n",
    "        normalize_row = input-max_row\n",
    "        expo_max = np.sum(np.exp(normalize_row),axis=1,keepdims=True)\n",
    "        expo_values = np.exp(normalize_row)/expo_max\n",
    "        self.output = expo_values\n",
    "        return self.output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f6c47",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7cf746ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_entropyLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1.0)\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "       \n",
    "        self.output = np.mean(sample_losses)\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c67c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([\n",
    "    [1, 0, 0],  # sample 1 → class 0\n",
    "    [0, 1, 0],  # sample 2 → class 1\n",
    "    [0, 0, 1]   # sample 3 → class 2\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87508809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(19.63061671445764)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = Cross_entropyLoss()\n",
    "cs.forward(output,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d6ccc",
   "metadata": {},
   "source": [
    "# Dense Layer with Activation, Softmax, Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0981175686269748\n"
     ]
    }
   ],
   "source": [
    "input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# Layer 1\n",
    "weights1 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "bias1 = np.zeros((1,3))\n",
    "\n",
    "# Layer 2\n",
    "weights2 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "bias2 = np.zeros((1,3))\n",
    "\n",
    "layer1=np.dot(input,weights1)+bias1\n",
    "# layer1 = ReLu(layer1)\n",
    "\n",
    "\n",
    "actSoft = Softmax()\n",
    "actSoft.forward(layer1)\n",
    "\n",
    "lossFun = Cross_entropyLoss()\n",
    "lossFun.forward(actSoft.output,[[1,0,0],[0,1,0],[1,0,0]])\n",
    "\n",
    "\n",
    "\n",
    "print(lossFun.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReluAct(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivativeRelu(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "inputs = np.array([10.0, 20.0, 30.0, 40.0])   \n",
    "weights = np.random.randn(3, 4)              \n",
    "biases = np.zeros(3)                        \n",
    "target = 5.0\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "dy_da = np.ones(3)\n",
    "\n",
    "for it in range(100):\n",
    "\n",
    "    z = weights.dot(inputs) + biases        \n",
    "    a = ReluAct(z)                        \n",
    "    y = a.sum()                           \n",
    "    loss = (y - target)**2\n",
    "\n",
    "    dL_dy = 2.0 * (y)             \n",
    "\n",
    "    da_dz = derivativeRelu(z)              \n",
    "    dL_da = dL_dy * dy_da                  \n",
    "    dL_dz = dL_da * da_dz                 \n",
    "\n",
    "    dL_dW = dL_dz[:, np.newaxis] * inputs[np.newaxis, :]   \n",
    "\n",
    "    dL_db = dL_dz.copy()                  \n",
    "\n",
    "    weights -= lr * dL_dW\n",
    "    biases  -= lr * dL_db\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        print(f\"iter {it:03d}  loss {loss:.6f}  y {y:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d9ce4",
   "metadata": {},
   "source": [
    "# Dense Layer + Backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce2db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d552de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluAct:\n",
    "    def forward(self, x):\n",
    "        self.output = np.maximum(0, x)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_dout):\n",
    "        dL_dinput = dL_dout * (self.output > 0).astype(float)\n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1.0)\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return np.mean(sample_losses)\n",
    "    def backward(self):\n",
    "        # dL_dypred = -2*(self.y_true-self.y_pred)\n",
    "        # self.output = dL_dypred\n",
    "        # return dL_dypred    \n",
    "        n_samples = self.y_true.shape[0]\n",
    "        dL_dypred = (self.y_pred - self.y_true) / n_samples\n",
    "        return dL_dypred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23e3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        max_row = np.max(x, axis=1, keepdims=True)\n",
    "        normalize_row = x - max_row\n",
    "        expo = np.exp(normalize_row)\n",
    "        self.output = expo / np.sum(expo, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        # y_true is one-hot labels\n",
    "        # combined with cross-entropy derivative\n",
    "        # dL/dz = softmax_output - y_true\n",
    "        dL_dinput = self.output - y_true\n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2cc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd3a4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * 0.01\n",
    "        self.biases = np.zeros(output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x            # shape: (batch, input_dim)\n",
    "        self.output = x @ self.weights.T + self.biases  # batch x output_dim\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dL_dout):\n",
    "        # Step 2: Gradients w.r.t weights and biases\n",
    "        self.dL_dW = np.dot(dL_dout[:, np.newaxis], self.input[np.newaxis, :])\n",
    "        self.dL_db = dL_dout.copy()\n",
    "        \n",
    "        # Step 3: Gradient w.r.t input for previous layer\n",
    "        dL_dinput = np.dot(self.weights.T, dL_dout)\n",
    "        \n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3818198",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,1,3) and (1,1,4) not aligned: 3 (dim 2) != 1 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m dL = softmax.backward(target)   \u001b[38;5;66;03m# (batch, output_dim)\u001b[39;00m\n\u001b[32m     20\u001b[39m dL = relu.backward(dL)          \u001b[38;5;66;03m# (batch, output_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m dL = \u001b[43mdense\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# returns (batch, input_dim)\u001b[39;00m\n\u001b[32m     22\u001b[39m         \u001b[38;5;66;03m# gradient w.r.t inputs\u001b[39;00m\n\u001b[32m     23\u001b[39m dense.weights -= lr * dense.dL_dW\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mDense.backward\u001b[39m\u001b[34m(self, dL_dout)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dL_dout):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Step 2: Gradients w.r.t weights and biases\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28mself\u001b[39m.dL_dW = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_dout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m.dL_db = dL_dout.copy()\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Step 3: Gradient w.r.t input for previous layer\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: shapes (3,1,3) and (1,1,4) not aligned: 3 (dim 2) != 1 (dim 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = np.array([[10.0, 20.0, 30.0, 40.0]]) \n",
    "target = np.array([[1.0, 0.0, 0.0]])           \n",
    "lr = 0.001\n",
    "\n",
    "dense = Dense(input_dim=4, output_dim=3)\n",
    "relu = ReluAct()\n",
    "softmax = Softmax()\n",
    "loss_func = CrossEntropyLoss()\n",
    "\n",
    "for it in range(100):\n",
    "    z = dense.forward(inputs)           \n",
    "    a = relu.forward(z)\n",
    "    y_pred = softmax.forward(a.T)       \n",
    "    loss = loss_func.forward(y_pred, target)\n",
    "        \n",
    "    dL = loss_func.backward()       # (batch, output_dim)\n",
    "    dL = softmax.backward(target)   # (batch, output_dim)\n",
    "    dL = relu.backward(dL)          # (batch, output_dim)\n",
    "    dL = dense.backward(dL)         # returns (batch, input_dim)\n",
    "            # gradient w.r.t inputs\n",
    "    dense.weights -= lr * dense.dL_dW\n",
    "    dense.biases  -= lr * dense.dL_db\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        print(f\"iter {it:03d}  loss {loss:.6f}  y_pred {y_pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
