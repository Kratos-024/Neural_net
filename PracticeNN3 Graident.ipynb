{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e61fed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18f23594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02652128, -0.0298474 , -0.00894195]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([1,2,3])\n",
    "weights = np.random.randn(3, 3) * 0.01\n",
    "bias = np.zeros((1,3))\n",
    "\n",
    "output=np.dot(input,weights)+bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f58487",
   "metadata": {},
   "source": [
    "Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c51f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluAct:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,inputs):\n",
    "         self.output=np.maximum(0,inputs)\n",
    "         return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c2ddb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([1,2,3])\n",
    "# weights = np.random.randn(3, 3) * 0.01\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "366010d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# weights = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ae984",
   "metadata": {},
   "source": [
    "# Batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00df55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# weights = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias = np.zeros((1,3))\n",
    "\n",
    "# output=np.dot(input,weights)+bias\n",
    "# print(output)\n",
    "# output = ReLu(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276db529",
   "metadata": {},
   "source": [
    "# Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e095d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# # Layer 1\n",
    "# weights1 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias1 = np.zeros((1,3))\n",
    "\n",
    "# # Layer 2\n",
    "# weights2 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "# bias2 = np.zeros((1,3))\n",
    "\n",
    "# layer1=np.dot(input,weights1)+bias1\n",
    "# layer1 = ReLu(layer1)\n",
    "# print(layer1)\n",
    "\n",
    "# layer2=np.dot(layer1,weights2)+bias2\n",
    "# output = ReLu(layer2)\n",
    "\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998c434",
   "metadata": {},
   "source": [
    "# Class of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98afd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights=0.01*np.random.randn(inputs,neurons)\n",
    "        self.bias=np.zeros((1,neurons))\n",
    "    def forward(self,input):\n",
    "        self.output = np.dot(input,self.weights)+self.bias\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551aa9",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9e09268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        max_row = np.max(input,axis=1,keepdims=True)\n",
    "        normalize_row = input-max_row\n",
    "        expo_max = np.sum(np.exp(normalize_row),axis=1,keepdims=True)\n",
    "        expo_values = np.exp(normalize_row)/expo_max\n",
    "        self.output = expo_values\n",
    "        return self.output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f6c47",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7cf746ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_entropyLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1.0)\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "       \n",
    "        self.output = np.mean(sample_losses)\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c67c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([\n",
    "    [1, 0, 0],  # sample 1 → class 0\n",
    "    [0, 1, 0],  # sample 2 → class 1\n",
    "    [0, 0, 1]   # sample 3 → class 2\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87508809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(19.63061671445764)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = Cross_entropyLoss()\n",
    "cs.forward(output,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d6ccc",
   "metadata": {},
   "source": [
    "# Dense Layer with Activation, Softmax, Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0981175686269748\n"
     ]
    }
   ],
   "source": [
    "input = np.array([[1,2,3],[3,4,5],[6,8,1]]) # 3x3\n",
    "# Layer 1\n",
    "weights1 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "bias1 = np.zeros((1,3))\n",
    "\n",
    "# Layer 2\n",
    "weights2 = np.random.randn(3, 3) * 0.01 # 3x3\n",
    "bias2 = np.zeros((1,3))\n",
    "\n",
    "layer1=np.dot(input,weights1)+bias1\n",
    "# layer1 = ReLu(layer1)\n",
    "\n",
    "\n",
    "actSoft = Softmax()\n",
    "actSoft.forward(layer1)\n",
    "\n",
    "lossFun = Cross_entropyLoss()\n",
    "lossFun.forward(actSoft.output,[[1,0,0],[0,1,0],[1,0,0]])\n",
    "\n",
    "\n",
    "\n",
    "print(lossFun.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReluAct(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivativeRelu(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "inputs = np.array([10.0, 20.0, 30.0, 40.0])   \n",
    "weights = np.random.randn(3, 4)              \n",
    "biases = np.zeros(3)                        \n",
    "target = 5.0\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "dy_da = np.ones(3)\n",
    "\n",
    "for it in range(100):\n",
    "\n",
    "    z = weights.dot(inputs) + biases        \n",
    "    a = ReluAct(z)                        \n",
    "    y = a.sum()                           \n",
    "    loss = (y - target)**2\n",
    "\n",
    "    dL_dy = 2.0 * (y)             \n",
    "\n",
    "    da_dz = derivativeRelu(z)              \n",
    "    dL_da = dL_dy * dy_da                  \n",
    "    dL_dz = dL_da * da_dz                 \n",
    "\n",
    "    dL_dW = dL_dz[:, np.newaxis] * inputs[np.newaxis, :]   \n",
    "\n",
    "    dL_db = dL_dz.copy()                  \n",
    "\n",
    "    weights -= lr * dL_dW\n",
    "    biases  -= lr * dL_db\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        print(f\"iter {it:03d}  loss {loss:.6f}  y {y:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d9ce4",
   "metadata": {},
   "source": [
    "# Dense Layer + Backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce2db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d552de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluAct:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.maximum(0, x)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_dout):\n",
    "        dL_dinput = dL_dout * (self.input > 0).astype(float)\n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1.0)\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return np.mean(sample_losses)\n",
    "    \n",
    "    def backward(self):\n",
    "        n_samples = self.y_true.shape[0]\n",
    "        dL_dy = (self.y_pred - self.y_true) / n_samples\n",
    "        return dL_dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        max_row = np.max(x, axis=1, keepdims=True)\n",
    "        normalize_row = x - max_row\n",
    "        expo = np.exp(normalize_row)\n",
    "        self.output = expo / np.sum(expo, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * 0.01\n",
    "        self.biases = np.zeros(output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x           \n",
    "        self.output = x @ self.weights.T + self.biases  \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dL_dout):\n",
    "        self.dL_dW = dL_dout.T @ self.input  \n",
    "        self.dL_db = np.sum(dL_dout, axis=0) \n",
    "        dL_dinput = dL_dout @ self.weights  \n",
    "        \n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3818198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 000  loss 0.895443  y_pred [0.40842677 0.29650596 0.29506728]\n",
      "iter 010  loss 0.030667  y_pred [0.9697986 0.0151007 0.0151007]\n",
      "iter 020  loss 0.015885  y_pred [0.98424052 0.00787974 0.00787974]\n",
      "iter 030  loss 0.010734  y_pred [0.98932323 0.00533838 0.00533838]\n",
      "iter 040  loss 0.008110  y_pred [0.99192326 0.00403837 0.00403837]\n",
      "iter 050  loss 0.006518  y_pred [0.99350363 0.00324819 0.00324819]\n",
      "iter 060  loss 0.005449  y_pred [0.99456616 0.00271692 0.00271692]\n",
      "iter 070  loss 0.004681  y_pred [0.99532968 0.00233516 0.00233516]\n",
      "iter 080  loss 0.004104  y_pred [0.9959049  0.00204755 0.00204755]\n",
      "iter 090  loss 0.003653  y_pred [0.99635386 0.00182307 0.00182307]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test data\n",
    "inputs = np.array([[10.0, 20.0, 30.0, 40.0]]) \n",
    "target = np.array([[1.0, 0.0, 0.0]])           \n",
    "lr = 0.001\n",
    "\n",
    "# Initialize layers\n",
    "dense = Dense(input_dim=4, output_dim=3)\n",
    "relu = ReluAct()\n",
    "softmax = Softmax()\n",
    "loss_func = CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for it in range(100):\n",
    "    # Forward pass\n",
    "    z = dense.forward(inputs)           # (1, 3)\n",
    "    a = relu.forward(z)                 # (1, 3)\n",
    "    y_pred = softmax.forward(a)         # (1, 3) - no transpose!\n",
    "    loss = loss_func.forward(y_pred, target)\n",
    "    \n",
    "    # Backward pass - combined softmax + cross-entropy gradient\n",
    "    dL = loss_func.backward()           # (1, 3)\n",
    "    dL = relu.backward(dL)              # (1, 3)\n",
    "    dL = dense.backward(dL)             # (1, 4)\n",
    "    \n",
    "    # Update parameters\n",
    "    dense.weights -= lr * dense.dL_dW\n",
    "    dense.biases  -= lr * dense.dL_db\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        print(f\"iter {it:03d}  loss {loss:.6f}  y_pred {y_pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad4217f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = np.array([2.0, 1.0, 0.1])\n",
    "y_pred = np.exp(z - np.max(z)) / np.sum(np.exp(z - np.max(z)))\n",
    "y_true = np.array([1, 0, 0])  # class 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "268c2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dy = -y_true / y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d237fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = y_pred.size\n",
    "J = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            J[i, j] = y_pred[i] * (1 - y_pred[i])\n",
    "        else:\n",
    "            J[i, j] = -y_pred[i] * y_pred[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b30d4141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.34099886  0.24243297  0.09856589]\n"
     ]
    }
   ],
   "source": [
    "dL_dz = J @ dL_dy\n",
    "print(dL_dz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fab94cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34099886,  0.24243297,  0.09856589])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b53f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
