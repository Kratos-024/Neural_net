{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80ad1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs.datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ad0f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d86e24b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51acf450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77094b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * 0.01\n",
    "        self.biases = np.zeros((1,output_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x           \n",
    "        self.output = np.dot(x, self.weights) + self.biases\n",
    "        \n",
    "    \n",
    "    def backward(self, dL_dout):\n",
    "        self.dweights = dL_dout.T @ self.input  \n",
    "        self.dbias = np.sum(dL_dout, axis=0) \n",
    "        dL_dinput = dL_dout @ self.weights  \n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d213a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluAct:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.maximum(0, x)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_dout):\n",
    "        dL_dinput = dL_dout * (self.input > 0).astype(float)\n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        max_row = np.max(x, axis=1, keepdims=True)\n",
    "        normalize_row = x - max_row\n",
    "        expo = np.exp(normalize_row)\n",
    "        self.output = expo / np.sum(expo, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        if y_true.ndim == 2 and y_true.shape[1] != y_pred.shape[1]:\n",
    "            raise ValueError(\"y_true shape does not match y_pred classes\")\n",
    "        if y_true.ndim == 1 or (y_true.ndim == 2 and y_true.shape[1] == 1):\n",
    "            y_one_hot = np.zeros_like(y_pred)\n",
    "            y_one_hot[np.arange(y_true.size), y_true.flatten()] = 1\n",
    "            y_true = y_one_hot\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-12, 1.0)\n",
    "        sample_losses = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return np.mean(sample_losses)\n",
    "    \n",
    "    def backward(self):\n",
    "        n_samples = self.y_true.shape[0]\n",
    "        dL_dy = (self.y_pred - self.y_true) / n_samples\n",
    "        return dL_dy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "044d3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizerRMSprop:\n",
    "    def __init__(self, layer,lr,betaDecay=0.9,epsilon=1e-8):\n",
    "        self.vt = 0\n",
    "        self.lr=lr\n",
    "        self.epsilon=epsilon\n",
    "        self.beta = betaDecay\n",
    "        self.layer = layer\n",
    "        self.vt_w = np.zeros_like(self.layer.weights)\n",
    "        self.vt_b = np.zeros_like(self.layer.biases)\n",
    "\n",
    "        \n",
    "    def update(self):\n",
    "        self.vt_w = self.beta * self.vt_w + (1-self.beta) * (self.layer.dweights**2)\n",
    "        self.vt_b = self.beta * self.vt_b + (1-self.beta)*(self.layer.dbias**2)\n",
    "\n",
    "        self.layer.weights = self.layer.weights - (self.lr)*(self.layer.dweights)/(np.sqrt(self.vt_w+self.epsilon))\n",
    "        self.layer.biases = self.layer.biases - (self.lr)*(self.layer.dbias)/(np.sqrt(self.vt_b+self.epsilon))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac4b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cafdc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizerMomentum:\n",
    "    def __init__(self, layer, lr, beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.layer = layer\n",
    "        self.vt_w = np.zeros_like(self.layer.weights)\n",
    "        self.vt_b = np.zeros_like(self.layer.biases)\n",
    "\n",
    "    def update(self):\n",
    "        self.vt_w = self.beta * self.vt_w +  self.layer.dweights\n",
    "        self.vt_b = self.beta * self.vt_b +  self.layer.dbias\n",
    "        \n",
    "        self.layer.weights = self.layer.weights - self.lr *self.vt_w\n",
    "        self.layer.biases = self.layer.biases -   self.lr *self.vt_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7665cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizerADAGrad:\n",
    "    def __init__(self, layer,lr):\n",
    "        self.vt = 0\n",
    "        self.lr=lr\n",
    "\n",
    "        self.layer = layer\n",
    "        self.vt_w = np.zeros_like(self.layer.weights)\n",
    "        self.vt_b = np.zeros_like(self.layer.biases)\n",
    "        self.left_w = None\n",
    "        self.left_b = None\n",
    "    def update(self):\n",
    "        self.vt_w = self.vt_w + self.layer.dweights**2\n",
    "        self.vt_b = self.vt_b + self.layer.dbiases**2\n",
    "        self.left_w =   (self.lr )/np.sqrt(self.vt_w+0.01)\n",
    "        self.left_b =   (self.lr )/np.sqrt(self.vt_b+0.01)\n",
    "\n",
    "        self.layer.weights = self.layer.weights - self.left_w*self.layer.dweights\n",
    "        self.layer.biases = self.layer.biases - self.left_b* self.layer.dbiases\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "536e8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SGDOptimizerADAM:\n",
    "#     def __init__(self, layer,lr,betaDecay1=0.01,betaDecay2=0.01):\n",
    "#         self.vt = 0\n",
    "#         self.lr=lr\n",
    "#         self.beta1 = betaDecay1\n",
    "#         self.beta2 = betaDecay2\n",
    "#         self.layer = layer\n",
    "#         self.wt_w = np.zeros_like(self.layer.weights)\n",
    "#         self.wt_b = np.zeros_like(self.layer.biases)        \n",
    "#         self.vt_w = np.zeros_like(self.layer.weights)\n",
    "#         self.vt_b = np.zeros_like(self.layer.biases)\n",
    "#         self.left_w = None\n",
    "#         self.left_b = None\n",
    "#     def update(self):\n",
    "#         self.wt_w = self.beta2*self.wt_w + (1-self.beta2)*self.layer.dweights\n",
    "#         self.wt_b = self.beta2*self.wt_b + (1-self.beta2)*self.layer.dbiases\n",
    "#         self.vt_w = self.beta1*self.vt_w + (1-self.beta1)*self.layer.dweights\n",
    "#         self.vt_b = self.beta1*self.vt_b + (1-self.beta1)*self.layer.dbiases\n",
    "#         self.left_w = (self.lr/(np.sqrt(self.vt_w+0.01)))*self.wt_w\n",
    "#         self.left_b = (self.lr/(np.sqrt(self.vt_b+0.01)))*self.wt_b\n",
    "#         self.layer.weights = self.layer.weights - self.left_w\n",
    "#         self.layer.biases = self.layer.biases - self.left_b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SGDOptimizerADAM:\n",
    "    def __init__(self, layer, lr, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.layer = layer\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m_w = np.zeros_like(layer.weights)  # first moment\n",
    "        self.m_b = np.zeros_like(layer.biases)\n",
    "        self.v_w = np.zeros_like(layer.weights)  # second moment\n",
    "        self.v_b = np.zeros_like(layer.biases)\n",
    "        self.t = 0  # timestep\n",
    "\n",
    "    def update(self):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * self.layer.dweights\n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * self.layer.dbias\n",
    "\n",
    "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (self.layer.dweights ** 2)\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (self.layer.dbias ** 2)\n",
    "\n",
    "        m_hat_w = self.m_w / (1 - self.beta1 ** self.t)\n",
    "        m_hat_b = self.m_b / (1 - self.beta1 ** self.t)\n",
    "        v_hat_w = self.v_w / (1 - self.beta2 ** self.t)\n",
    "        v_hat_b = self.v_b / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # Update parameters\n",
    "        self.layer.weights -= self.lr * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "        self.layer.biases -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560503e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class SGDOptimizer:\n",
    "    def __init__(self,betaDecay,learning_rate=0.001):\n",
    "        self.lr = learning_rate\n",
    "        self.beta = betaDecay\n",
    "        self.momentum = None  \n",
    "    def update(self,layer):\n",
    "        if self.beta==0:\n",
    "            layer.weights = layer.weights - self.lr * layer.dweights \n",
    "            layer.biases = layer.biases - self.lr * layer.dbias\n",
    "        else:\n",
    "            if self.momentum is None:\n",
    "                self.momentum = SGDOptimizerMomentum(layer, self.lr, self.beta)\n",
    "            self.momentum.update()\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e25fe12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculate accuracy\"\"\"\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    return np.mean(y_pred_classes == y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29b27931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#      X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1990a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = nnfs.datasets.spiral_data(samples=1000, classes=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b608c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Dense(2, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825cacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3000,2) and (64,3) not aligned: 2 (dim 1) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m accuracies = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     z = \u001b[43mdense\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     a = relu.forward(z)\n\u001b[32m     15\u001b[39m     y_pred = softmax.forward(a)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mDense.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.input = x           \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mself\u001b[39m.output = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.biases\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output\n",
      "\u001b[31mValueError\u001b[39m: shapes (3000,2) and (64,3) not aligned: 2 (dim 1) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dense = Dense(input_dim=3, output_dim=64)\n",
    "relu = ReluAct()\n",
    "softmax = Softmax()\n",
    "loss_func = CrossEntropyLoss()\n",
    "optimizer = SGDOptimizer(learning_rate=0.01,betaDecay=0)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for it in range(10000):\n",
    "    z = dense.forward(X)\n",
    "    a = relu.forward(z)\n",
    "    y_pred = softmax.forward(a)\n",
    "    loss = loss_func.forward(y_pred, y)\n",
    "    \n",
    "    dL = loss_func.backward()\n",
    "    dL = relu.backward(dL)\n",
    "    dL = dense.backward(dL)\n",
    "    optimizer.update(dense)\n",
    "    \n",
    "    acc = accuracy(y_pred, y.flatten())\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    if it % 500 == 0:\n",
    "        print(f\"iter {it:05d}  loss {loss:.6f}  acc {acc:.4f}\")\n",
    "\n",
    "print(\"Training complete!, Without Momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3cf2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m accuracies = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     z = \u001b[43mdense1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     a = relu.forward(z)\n\u001b[32m     16\u001b[39m     new_z = dense2.forward(a)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mDense.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.input = x           \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mself\u001b[39m.output = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m + \u001b[38;5;28mself\u001b[39m.biases  \n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "dense1 = Dense(input_dim=3, output_dim=64)\n",
    "dense2 = Dense(input_dim=64, output_dim=3)\n",
    "\n",
    "relu = ReluAct()\n",
    "softmax = Softmax()\n",
    "loss_func = CrossEntropyLoss()\n",
    "optimizer = SGDOptimizer(learning_rate=0.01,betaDecay=0.9)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for it in range(10000):\n",
    "    z = dense1.forward(X)\n",
    "    a = relu.forward(z)\n",
    "    new_z = dense2.forward(a)\n",
    "    y_pred = softmax.forward(a)\n",
    "    loss = loss_func.forward(y_pred, y)\n",
    "    \n",
    "    dL = loss_func.backward()\n",
    "    dL = dense2.backward(dL)\n",
    "    dL = relu.backward(dL)\n",
    "    dL = dense1.backward(dL)\n",
    "    optimizer.update(dense1)\n",
    "    optimizer.update(dense2)\n",
    "\n",
    "    acc = accuracy(y_pred, y.flatten())\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    if it % 500 == 0:\n",
    "        print(f\"iter {it:05d}  loss {loss:.6f}  acc {acc:.4f}\")\n",
    "\n",
    "print(\"Training complete!, With Momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682aa41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = nnfs.datasets.spiral_data(samples=1000, classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# z = dense.forward(X_test)\n",
    "# a = relu.forward(z)\n",
    "# y_pred = softmax.forward(a)\n",
    "# test_loss = loss_func.forward(y_pred, y_test)\n",
    "# test_acc = accuracy(y_pred, y_test.flatten())\n",
    "# print(f\"\\nTest loss: {test_loss:.6f}  Test acc: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = 0.02\n",
    "# x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "# y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Z = dense.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Z = relu.forward(Z)\n",
    "# Z = softmax.forward(Z)\n",
    "# Z = np.argmax(Z, axis=1).reshape(xx.shape)\n",
    "\n",
    "# plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "# plt.title('Decision Boundary')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# axes[0].plot(losses)\n",
    "# axes[0].set_xlabel('Iteration')\n",
    "# axes[0].set_ylabel('Loss')\n",
    "# axes[0].set_title('Training Loss')\n",
    "# axes[0].grid(True)\n",
    "\n",
    "# axes[1].plot(accuracies)\n",
    "# axes[1].set_xlabel('Iteration')\n",
    "# axes[1].set_ylabel('Accuracy')\n",
    "# axes[1].set_title('Training Accuracy')\n",
    "# axes[1].grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc365f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m12\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m axes[\u001b[32m0\u001b[39m].plot(\u001b[43mtest_loss\u001b[49m)\n\u001b[32m      4\u001b[39m axes[\u001b[32m0\u001b[39m].set_xlabel(\u001b[33m'\u001b[39m\u001b[33mIteration\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m axes[\u001b[32m0\u001b[39m].set_ylabel(\u001b[33m'\u001b[39m\u001b[33mLoss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_loss' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(test_loss)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Testing Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(test_acc)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Testing Accuracy')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
